{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7fb4d2",
   "metadata": {},
   "source": [
    "Reproduce the tutorial analysis with data provided by basenji tutorial\n",
    "\n",
    "The input you bring to the pipeline is:\n",
    "* BigWig coverage tracks\n",
    "* Genome FASTA file\n",
    "\n",
    "They provide a machine learning friendly simplified version of hg19 FASTA file;\n",
    "\n",
    "BigWig data was a few CAGE datasets from FANTOM5 related to heart biology and processed by:\n",
    "1. Aligning with Bowtie2 with very sensitive alignment parameters.\n",
    "2. Distributing multi-mapping reads and estimating genomic coverage with [bam_cov.py](https://github.com/calico/basenji/blob/master/bin/bam_cov.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a86a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before work dir: /Users/library/Documents/ML_project/basenji_genomics_ai/experiments/reproduction/notebooks\n",
      "after work dir: /Users/library/Documents/ML_project/basenji_genomics_ai/experiments/reproduction\n"
     ]
    }
   ],
   "source": [
    "# confirm the download directory\n",
    "import os\n",
    "\n",
    "print(f\"before work dir: {os.getcwd()}\")\n",
    "# change to parent directory\n",
    "os.chdir(\"..\")\n",
    "print(f\"after work dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   729  100   729    0     0    893      0 --:--:-- --:--:-- --:--:--   916\n"
     ]
    }
   ],
   "source": [
    "# download fasta files\n",
    "import subprocess\n",
    "if not os.path.isfile('data/hg19.ml.fa'):\n",
    "    subprocess.call('curl -o data/hg19.ml.fa https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa', shell=True)\n",
    "    subprocess.call('curl -o data/hg19.ml.fa.fai https://storage.googleapis.com/basenji_tutorial_data/hg19.ml.fa.fai', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1563M  100 1563M    0     0  16.8M      0  0:01:32  0:01:32 --:--:-- 18.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1236M  100 1236M    0     0  17.5M      0  0:01:10  0:01:10 --:--:-- 19.4M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3484M  100 3484M    0     0  11.3M      0  0:05:06  0:05:06 --:--:-- 7945k\n"
     ]
    }
   ],
   "source": [
    "# download BigWig files\n",
    "if not os.path.isfile('data/CNhs11760.bw'):\n",
    "    subprocess.call('curl -o data/CNhs11760.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs11760.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12843.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12843.bw', shell=True)\n",
    "    subprocess.call('curl -o data/CNhs12856.bw https://storage.googleapis.com/basenji_tutorial_data/CNhs12856.bw', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed4b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [['index','identifier','file','clip','sum_stat','description']]\n",
    "lines.append(['0', 'CNhs11760', 'data/CNhs11760.bw', '384', 'sum', 'aorta'])\n",
    "lines.append(['1', 'CNhs12843', 'data/CNhs12843.bw', '384', 'sum', 'artery'])\n",
    "lines.append(['2', 'CNhs12856', 'data/CNhs12856.bw', '384', 'sum', 'pulmonic_valve'])\n",
    "\n",
    "samples_out = open('data/heart_wigs.txt', 'w')\n",
    "for line in lines:\n",
    "    print('\\t'.join(line), file=samples_out)\n",
    "samples_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608719d1",
   "metadata": {},
   "source": [
    "Next, we want to choose genomic sequences to form batches for stochastic gradient descent, divide them into training/validation/test sets, and construct TFRecords to provide to downstream programs.\n",
    "\n",
    "The script [basenji_data.py](https://github.com/calico/basenji/blob/master/bin/basenji_data.py) implements this procedure.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| -s | 0.1 | Down-sample the genome to 10% to speed things up here. |\n",
    "| -g | data/unmap_macro.bed | Dodge large-scale unmappable regions like assembly gaps. |\n",
    "| -l | 131072 | Sequence length. |\n",
    "| --local | True | Run locally, as opposed to on my SLURM scheduler. |\n",
    "| -o | data/heart_l131k | Output directory |\n",
    "| -p | 8 | Uses multiple concourrent processes to read/write. |\n",
    "| -t | .1 | Hold out 10% sequences for testing. |\n",
    "| -v | .1 | Hold out 10% sequences for validation. |\n",
    "| -w | 128 | Pool the nucleotide-resolution values to 128 bp bins. |\n",
    "| fasta_file| data/hg19.ml.fa | FASTA file to extract sequences from. |\n",
    "| targets_file | data/heart_wigs.txt | Target samples table with BigWig paths. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee53d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride_train 1 converted to 131072.000000\n",
      "stride_test 1 converted to 131072.000000\n",
      "Contigs divided into\n",
      " Train:  4701 contigs, 2169074921 nt (0.8005)\n",
      " Valid:   572 contigs,  270358978 nt (0.0998)\n",
      " Test:    584 contigs,  270330829 nt (0.0998)\n",
      "python basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs11760.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/0.h5\n",
      "python basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12843.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/1.h5\n",
      "python basenji_data_read.py -w 128 -u sum -c 384.000000 -s 1.000000 data/CNhs12856.bw data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov/2.h5\n",
      "Targets sum: 160677.829\n",
      "Targets sum: 276909.169\n",
      "Targets sum: 743743.941\n",
      "python basenji_data_write.py -s 0 -e 256 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-0.tfr\n",
      "python basenji_data_write.py -s 256 -e 512 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-1.tfr\n",
      "python basenji_data_write.py -s 512 -e 768 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-2.tfr\n",
      "python basenji_data_write.py -s 768 -e 1024 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-3.tfr\n",
      "python basenji_data_write.py -s 1024 -e 1280 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-4.tfr\n",
      "python basenji_data_write.py -s 1280 -e 1499 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/train-5.tfr\n",
      "python basenji_data_write.py -s 1499 -e 1679 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/valid-0.tfr\n",
      "python basenji_data_write.py -s 1679 -e 1858 --umap_clip 1.000000 -x 0 data/hg19.ml.fa data/heart_l131k/sequences.bed data/heart_l131k/seqs_cov data/heart_l131k/tfrecords/test-0.tfr\n",
      "2025-09-28 15:44:53.533874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:53.694188: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:53.792044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:53.834557: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:53.877884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:53.897851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:54.087267: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-28 15:44:54.286574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# process the data into a dataset\n",
    "! python basenji_data.py -s .1 -g data/unmap_macro.bed -l 131072 --local -o data/heart_l131k -p 8 -t .1 -v .1 -w 128 data/hg19.ml.fa data/heart_wigs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c1734",
   "metadata": {},
   "source": [
    "Now, data/heart_l131k contains relevant data for training.\n",
    "\n",
    "*contigs.bed* contains the original large contiguous regions from which training sequences were taken (possibly strided).\n",
    "\n",
    "*sequences.bed* contains the train/valid/test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f0f10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179 test\n",
      "1499 train\n",
      " 180 valid\n"
     ]
    }
   ],
   "source": [
    "! cut -f4 data/heart_l131k/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac116226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2\t140425791\t140556863\ttrain\n",
      "chr16\t27143973\t27275045\ttrain\n",
      "chr14\t72972403\t73103475\ttrain\n"
     ]
    }
   ],
   "source": [
    "! head -n3 data/heart_l131k/sequences.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f685491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr11\t116271608\t116402680\tvalid\n",
      "chr2\t26239628\t26370700\tvalid\n",
      "chr6\t102241368\t102372440\tvalid\n"
     ]
    }
   ],
   "source": [
    "! grep valid data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "638e0be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr14\t76007464\t76138536\ttest\n",
      "chr13\t99391330\t99522402\ttest\n",
      "chr15\t52280212\t52411284\ttest\n"
     ]
    }
   ],
   "source": [
    "! grep test data/heart_l131k/sequences.bed | head -n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8220b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 library  staff   7205322 Sep 28 15:45 data/heart_l131k/tfrecords/test-0.tfr\n",
      "-rw-r--r--  1 library  staff  10314025 Sep 28 15:45 data/heart_l131k/tfrecords/train-0.tfr\n",
      "-rw-r--r--  1 library  staff  10276124 Sep 28 15:45 data/heart_l131k/tfrecords/train-1.tfr\n",
      "-rw-r--r--  1 library  staff  10317142 Sep 28 15:45 data/heart_l131k/tfrecords/train-2.tfr\n",
      "-rw-r--r--  1 library  staff  10318764 Sep 28 15:45 data/heart_l131k/tfrecords/train-3.tfr\n",
      "-rw-r--r--  1 library  staff  10302540 Sep 28 15:45 data/heart_l131k/tfrecords/train-4.tfr\n",
      "-rw-r--r--  1 library  staff   8828267 Sep 28 15:45 data/heart_l131k/tfrecords/train-5.tfr\n",
      "-rw-r--r--  1 library  staff   7244099 Sep 28 15:45 data/heart_l131k/tfrecords/valid-0.tfr\n"
     ]
    }
   ],
   "source": [
    "! ls -l data/heart_l131k/tfrecords/*.tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc4269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenji_genomics_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
